# Página n16 - Análise Multivariada

## Análise de Componentes Principais (PCA)

### Objetivo
Reduzir dimensionalidade mantendo máxima variabilidade.

### Primeira Componente Principal
$$\mathbf{a}_1 = \arg\max_{\|\mathbf{a}\|=1} \text{Var}(\mathbf{a}^T \mathbf{X})$$

### Solução
$\mathbf{a}_1$ é o autovetor correspondente ao maior autovalor de $\boldsymbol{\Sigma}$.

### Componentes Sucessivas
$$\mathbf{a}_k = \arg\max_{\|\mathbf{a}\|=1, \mathbf{a}^T\mathbf{a}_j=0, j<k} \text{Var}(\mathbf{a}^T \mathbf{X})$$

## Análise Discriminante

### Linear Discriminant Analysis (LDA)

### Função Discriminante Linear
$$d_i(\mathbf{x}) = \mathbf{x}^T \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_i - \frac{1}{2} \boldsymbol{\mu}_i^T \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_i + \log \pi_i$$

onde:
- $\boldsymbol{\mu}_i$ = vetor de médias da classe $i$
- $\boldsymbol{\Sigma}$ = matriz de covariância comum
- $\pi_i$ = probabilidade a priori da classe $i$

### Regra de Classificação
Classificar $\mathbf{x}$ na classe $i$ se $d_i(\mathbf{x}) = \max_j d_j(\mathbf{x})$.

## Análise de Agrupamento

### Distância Euclidiana
$$d(\mathbf{x}_i, \mathbf{x}_j) = \sqrt{\sum_{k=1}^p (x_{ik} - x_{jk})^2}$$

### Algoritmo K-means
1. Escolher $k$ centros iniciais
2. Atribuir cada ponto ao centro mais próximo
3. Recalcular centros como médias dos grupos
4. Repetir até convergência

### Critério de Inércia
$$W = \sum_{i=1}^k \sum_{\mathbf{x} \in C_i} \|\mathbf{x} - \boldsymbol{\mu}_i\|^2$$
