\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[brazil]{babel}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\geometry{margin=2.5cm}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue,urlcolor=blue}
\usepackage{enumitem}

\newtheorem{theorem}{Teorema}
\newtheorem{definition}{Definição}
\newtheorem{proposition}{Proposição}
\newtheorem{corollary}{Corolário}

\title{Resumo Teórico - Inferência Estatística}
\author{Curso de Inferência Estatística}
\date{Outubro 2025}

\begin{document}
\maketitle

\section{Convergência Estocástica}

\subsection{Resultados Fundamentais de Limites}

\begin{proposition}
Para $a \in \mathbb{R}$:
\begin{align}
\lim_{n \to \infty} n^{1/n} &= 1 \\
\lim_{n \to \infty} \left(1 + \frac{a}{n}\right)^n &= e^a
\end{align}
\end{proposition}

\subsection{Tipos de Convergência}

\begin{definition}[Convergência em Probabilidade]
$X_n \xrightarrow{P} X$ se $\lim_{n \to \infty} P(|X_n - X| > \varepsilon) = 0$ para todo $\varepsilon > 0$.
\end{definition}

\begin{definition}[Convergência Quase Certa]
$X_n \xrightarrow{a.s.} X$ se $P(\lim_{n \to \infty} X_n = X) = 1$.
\end{definition}

\begin{definition}[Convergência em Distribuição]
$X_n \xrightarrow{d} X$ se $\lim_{n \to \infty} F_{X_n}(x) = F_X(x)$ para todo ponto de continuidade de $F_X$.
\end{definition}

\begin{proposition}[Hierarquia de Convergências]
\begin{enumerate}
\item Convergência quase certa $\Rightarrow$ Convergência em probabilidade
\item Convergência em probabilidade $\Rightarrow$ Convergência em distribuição
\item Convergência em $L^p$ $\Rightarrow$ Convergência em probabilidade
\end{enumerate}
\end{proposition}

\section{Teorema Central do Limite}

\begin{theorem}[TCL]
Se $X_1, X_2, \ldots$ são i.i.d. com $E[X_i] = \mu$ e $\text{Var}(X_i) = \sigma^2 < \infty$, então:
$$\frac{\sqrt{n}(\bar{X}_n - \mu)}{\sigma} \xrightarrow{d} N(0,1)$$
\end{theorem}

\begin{corollary}[Aproximação Normal para Binomial]
Se $X \sim \text{Binomial}(n,p)$ com $n$ grande:
$$\frac{X - np}{\sqrt{np(1-p)}} \xrightarrow{d} N(0,1)$$
\end{corollary}

\section{Método Delta}

\begin{theorem}[Método Delta]
Se $\sqrt{n}(X_n - \theta) \xrightarrow{d} N(0, \sigma^2)$ e $g$ é diferenciável em $\theta$ com $g'(\theta) \neq 0$, então:
$$\sqrt{n}(g(X_n) - g(\theta)) \xrightarrow{d} N(0, \sigma^2[g'(\theta)]^2)$$
\end{theorem}

\section{Estimação Pontual}

\subsection{Propriedades dos Estimadores}

\begin{definition}[Não-Viciado]
$T$ é não-viciado para $\theta$ se $E[T] = \theta$.
\end{definition}

\begin{definition}[Consistente]
$T_n$ é consistente para $\theta$ se $T_n \xrightarrow{P} \theta$.
\end{definition}

\begin{definition}[Eficiente]
Entre estimadores não-viciados, o mais eficiente tem menor variância.
\end{definition}

\subsection{Métodos de Estimação}

\textbf{Método dos Momentos}: $\frac{1}{n}\sum_{i=1}^n X_i^k = E[X^k]$

\textbf{Máxima Verossimilhança}: $\hat{\theta} = \arg\max_\theta L(\theta)$

\section{Intervalos de Confiança}

\begin{definition}[Intervalo de Confiança]
$(L, U)$ é um IC para $\theta$ com nível $(1-\alpha) \times 100\%$ se $P(L \leq \theta \leq U) = 1 - \alpha$.
\end{definition}

\subsection{Intervalos Principais}

\textbf{Média (variância conhecida)}: $\bar{X} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}$

\textbf{Média (variância desconhecida)}: $\bar{X} \pm t_{\alpha/2, n-1} \frac{S}{\sqrt{n}}$

\textbf{Proporção}: $\hat{p} \pm z_{\alpha/2} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$

\textbf{Variância}: $\frac{(n-1)S^2}{\chi^2_{\alpha/2, n-1}} \leq \sigma^2 \leq \frac{(n-1)S^2}{\chi^2_{1-\alpha/2, n-1}}$

\section{Testes de Hipóteses}

\subsection{Conceitos Fundamentais}

\begin{definition}[Tipos de Erro]
\begin{itemize}
\item \textbf{Erro Tipo I}: Rejeitar $H_0$ quando verdadeira ($\alpha$)
\item \textbf{Erro Tipo II}: Aceitar $H_0$ quando falsa ($\beta$)
\end{itemize}
\end{definition}

\begin{definition}[Poder do Teste]
$\pi(\theta) = 1 - \beta(\theta) = P(\text{Rejeitar } H_0 | \theta)$
\end{definition}

\subsection{Testes Principais}

\textbf{Teste Z}: $Z = \frac{\bar{X} - \mu_0}{\sigma/\sqrt{n}} \sim N(0,1)$

\textbf{Teste t}: $t = \frac{\bar{X} - \mu_0}{S/\sqrt{n}} \sim t_{n-1}$

\textbf{Teste para Proporção}: $Z = \frac{\hat{p} - p_0}{\sqrt{p_0(1-p_0)/n}}$

\section{Testes de Aderência}

\subsection{Teste Qui-Quadrado}

\textbf{Estatística}: $\chi^2 = \sum_{i=1}^k \frac{(O_i - E_i)^2}{E_i} \sim \chi^2_{k-1-m}$

onde $m$ = número de parâmetros estimados.

\section{Análise de Variância}

\subsection{ANOVA de Um Fator}

\textbf{Modelo}: $Y_{ij} = \mu + \alpha_i + \varepsilon_{ij}$

\textbf{Hipóteses}: $H_0: \alpha_1 = \cdots = \alpha_k = 0$

\textbf{Estatística}: $F = \frac{\text{QME}}{\text{QMR}} \sim F_{k-1, N-k}$

\section{Regressão Linear}

\subsection{Regressão Simples}

\textbf{Modelo}: $Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$

\textbf{Estimadores}:
$$\hat{\beta}_1 = \frac{S_{XY}}{S_{XX}}, \quad \hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X}$$

\textbf{Distribuições}:
$$\hat{\beta}_1 \sim N\left(\beta_1, \frac{\sigma^2}{S_{XX}}\right)$$

\subsection{Regressão Múltipla}

\textbf{Modelo}: $\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}$

\textbf{Estimador}: $\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$

\textbf{Variância}: $\text{Var}(\hat{\boldsymbol{\beta}}) = \sigma^2(\mathbf{X}^T\mathbf{X})^{-1}$

\section{Análise de Resíduos}

\subsection{Diagnósticos}

\textbf{Resíduos Padronizados}: $e_i^* = \frac{e_i}{S\sqrt{1-h_{ii}}}$

\textbf{Distância de Cook}: $D_i = \frac{e_i^2}{(p+1)S^2} \cdot \frac{h_{ii}}{(1-h_{ii})^2}$

\textbf{Leverage}: $h_{ii} > \frac{2(p+1)}{n}$ indica alta influência.

\section{Testes Não-Paramétricos}

\subsection{Teste de Wilcoxon}

\textbf{Estatística}: $U = n_1 n_2 + \frac{n_1(n_1+1)}{2} - R_1$

\subsection{Teste de Kruskal-Wallis}

\textbf{Estatística}: $H = \frac{12}{N(N+1)} \sum_{i=1}^k \frac{R_i^2}{n_i} - 3(N+1) \sim \chi^2_{k-1}$

\subsection{Correlação de Spearman}

\textbf{Coeficiente}: $r_s = 1 - \frac{6\sum d_i^2}{n(n^2-1)}$

\section{Análise de Séries Temporais}

\subsection{Modelos ARIMA}

\textbf{AR(1)}: $X_t = \phi X_{t-1} + \varepsilon_t$ (estacionário se $|\phi| < 1$)

\textbf{MA(1)}: $X_t = \varepsilon_t + \theta \varepsilon_{t-1}$

\textbf{ARMA(p,q)}: Combinação de AR(p) e MA(q)

\section{Análise de Sobrevivência}

\subsection{Funções Fundamentais}

\textbf{Sobrevivência}: $S(t) = P(T > t)$

\textbf{Risco}: $h(t) = \lim_{\Delta t \to 0} \frac{P(t \leq T < t + \Delta t | T \geq t)}{\Delta t}$

\textbf{Relação}: $S(t) = \exp\left(-\int_0^t h(u) du\right)$

\subsection{Estimador de Kaplan-Meier}

$$\hat{S}(t) = \prod_{t_i \leq t} \left(1 - \frac{d_i}{n_i}\right)$$

\section{Análise Multivariada}

\subsection{Componentes Principais}

Primeira componente: $\mathbf{a}_1 = \arg\max_{\|\mathbf{a}\|=1} \text{Var}(\mathbf{a}^T \mathbf{X})$

\subsection{Análise Discriminante}

\textbf{Função Discriminante}:
$$d_i(\mathbf{x}) = \mathbf{x}^T \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_i - \frac{1}{2} \boldsymbol{\mu}_i^T \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_i + \log \pi_i$$

\section{Métodos de Bootstrap}

\subsection{Bootstrap Não-Paramétrico}

\textbf{Variância Bootstrap}: $\text{Var}^*(\hat{\theta}^*) = \frac{1}{B-1} \sum_{b=1}^B (\hat{\theta}_b^* - \bar{\theta}^*)^2$

\textbf{IC Percentil}: $CI_{1-\alpha} = [\hat{\theta}_{(\alpha/2)}, \hat{\theta}_{(1-\alpha/2)}]$

\section{Validação Cruzada e Seleção de Modelos}

\subsection{Validação Cruzada}

\textbf{K-Fold}: $CV_{(k)} = \frac{1}{k} \sum_{i=1}^k \text{MSE}_i$

\subsection{Critérios de Seleção}

\textbf{AIC}: $AIC = -2 \log L + 2p$

\textbf{BIC}: $BIC = -2 \log L + p \log n$

\textbf{Mallow's Cp}: $C_p = \frac{SSE_p}{S^2} - n + 2p$

\subsection{Regularização}

\textbf{Ridge}: $\hat{\boldsymbol{\beta}} = \arg\min_{\boldsymbol{\beta}} \{\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda \|\boldsymbol{\beta}\|^2\}$

\textbf{Lasso}: $\hat{\boldsymbol{\beta}} = \arg\min_{\boldsymbol{\beta}} \{\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda \|\boldsymbol{\beta}\|_1\}$

\end{document}
