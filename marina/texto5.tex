\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage{amsmath, amssymb}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{array}
\usepackage{indentfirst}
\usepackage{setspace}

% Layout
\geometry{left=3cm, right=2cm, top=3cm, bottom=2cm}
\onehalfspacing

\title{Teste de Hipótese em ANOVA para Delineamento Inteiramente Casualizado}
\author{Marina Oliveira Cunha \\ Universidade Federal de Pernambuco}
\date{\today}

\begin{document}
\maketitle

\section{Introdução}

A Análise de Variância (ANOVA), desenvolvida por Fisher, permite testar a igualdade das médias de múltiplos tratamentos. No Delineamento Inteiramente Casualizado (DIC), os tratamentos são distribuídos aleatoriamente às unidades experimentais. O objetivo é testar:
\[
H_0: \mu_1 = \cdots = \mu_t
\quad\text{versus}\quad
H_1: \text{pelo menos uma média difere}.
\]

A ANOVA de um fator pode ser vista como caso particular do modelo linear geral \(y = X\beta + \epsilon\), onde a matriz de delineamento \(\mathbf{X}\) tem estrutura específica determinada pelo arranjo experimental. O modelo DIC assume \(y_{ij} = \mu + \tau_i + \epsilon_{ij}\) para \(i = 1, \ldots, t\) tratamentos e \(j = 1, \ldots, r\) repetições, sendo \(\epsilon_{ij} \sim N(0,\sigma^2)\) independentes. Este trabalho fundamenta o teste F via decomposição de soma de quadrados e aplicação do Teorema de Cochran, que estabelece condições para que somas de quadrados sigam distribuições qui-quadrado independentes, essencial para a validade estatística exata do teste.

\section{Modelo Estatístico}

O modelo do DIC, para $i = 1, \ldots, t$ tratamentos e $j = 1, \ldots, r$ repetições, é:
\[
y_{ij} = \mu + \tau_i + \epsilon_{ij}, \qquad
\epsilon_{ij} \sim N(0,\sigma^2) \text{ independentes},
\]
onde $\mu$ é a média geral e $\tau_i$ o efeito do tratamento $i$. Para garantir identificabilidade, impomos a restrição $\sum_{i=1}^t \tau_i = 0$. As hipóteses são:
\[
H_0: \tau_1=\cdots=\tau_t=0
\quad\text{e}\quad
H_1: \exists\, i: \tau_i\neq 0.
\]

Em notação matricial, temos $\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$, onde $\mathbf{y}$ é o vetor de observações, $\mathbf{X}$ é a matriz de delineamento com estrutura de blocos correspondente aos tratamentos, $\boldsymbol{\beta} = (\mu, \tau_1, \ldots, \tau_t)^T$ e $\boldsymbol{\epsilon} \sim N(\mathbf{0}, \sigma^2\mathbf{I})$. Como $\mu_i = \mu + \tau_i$ e $\sum_{i=1}^t \tau_i = 0$, segue que $\mu_1 = \cdots = \mu_t \Longleftrightarrow \tau_1 = \cdots = \tau_t = 0$, estabelecendo a equivalência entre formular as hipóteses em termos de médias ou de efeitos.

\section{Pressupostos e Diagnósticos}

A validade do teste F com distribuições exatas requer os seguintes pressupostos:  
(i) \textbf{normalidade}: $\epsilon_{ij} \sim N(0,\sigma^2)$;  
(ii) \textbf{homogeneidade das variâncias}: $\text{Var}(\epsilon_{ij}) = \sigma^2$ para todo $i,j$;  
(iii) \textbf{independência}: as observações $\epsilon_{ij}$ são mutuamente independentes.  

Estes pressupostos são necessários para que as somas de quadrados sigam distribuições qui-quadrado exatas e independentes via Teorema de Cochran. Em particular, a normalidade e independência garantem que projeções ortogonais de vetores normais resultem em distribuições $\chi^2$ independentes. Testes como Shapiro–Wilk, Breusch–Pagan e gráficos de resíduos podem ser utilizados para avaliar tais pressupostos. A violação destes pressupostos compromete a validade das distribuições exatas e pode exigir métodos alternativos.

\section{Partição da Soma de Quadrados}

A partir da decomposição
\[
y_{ij}-\bar y = (y_{ij}-\bar y_i) + (\bar y_i-\bar y),
\]
obtemos
\[
SQ_T = SQ_E + SQ_A,
\]
onde $SQ_T = \sum_{i=1}^t \sum_{j=1}^r (y_{ij}-\bar y)^2$ é a soma de quadrados total, $SQ_A = r\sum_{i=1}^t (\bar y_i-\bar y)^2$ é a soma de quadrados entre tratamentos, e $SQ_E = \sum_{i=1}^t \sum_{j=1}^r (y_{ij}-\bar y_i)^2$ é a soma de quadrados residual. A validade da decomposição decorre da ortogonalidade dos componentes: $\sum_{i=1}^t \sum_{j=1}^r (y_{ij}-\bar y_i)(\bar y_i-\bar y) = 0$, pois $\sum_{j=1}^r (y_{ij}-\bar y_i) = 0$ para cada $i$. Esta ortogonalidade é essencial para garantir a independência entre $SQ_A$ e $SQ_E$ sob normalidade. Aqui, $SQ_A$ mede a variação entre as médias dos tratamentos e $SQ_E$ a variação dentro dos tratamentos.

\section{Distribuições das Somas de Quadrados}

Para justificar as distribuições qui-quadrado das somas de quadrados, utilizamos o Teorema de Cochran. Este teorema estabelece condições sob as quais somas de quadrados de variáveis aleatórias normais seguem distribuições qui-quadrado independentes, sendo essencial para a validade exata do teste F.

\subsection{Teorema de Cochran}

Sob normalidade e independência, se $\mathbf{y} \sim N(\boldsymbol{\mu}, \sigma^2\mathbf{I})$ e $\mathbf{Q}_1, \ldots, \mathbf{Q}_k$ são matrizes simétricas idempotentes tais que $\mathbf{Q}_1 + \cdots + \mathbf{Q}_k = \mathbf{I}$ e $\text{rank}(\mathbf{Q}_i) = r_i$, então:
\begin{enumerate}
    \item $\mathbf{y}^T\mathbf{Q}_i\mathbf{y}/\sigma^2 \sim \chi^2_{r_i}(\delta_i)$ são independentes, onde $\delta_i$ são parâmetros de não-centralidade
    \item Se $\mathbf{Q}_i\boldsymbol{\mu} = \mathbf{0}$, temos distribuições qui-quadrado centrais (não-centralidade zero)
\end{enumerate}

A intuição é que, quando decompomos o vetor de observações em componentes ortogonais (via matrizes de projeção que somam a identidade), cada componente gera uma soma de quadrados que segue distribuição qui-quadrado independente.

\subsection{Aplicação ao Modelo DIC}

No modelo DIC, podemos escrever as somas de quadrados na forma matricial $\mathbf{y}^T\mathbf{Q}\mathbf{y}$, onde $\mathbf{Q}$ são matrizes de projeção específicas. A decomposição ortogonal da Seção 4 garante que existem matrizes $\mathbf{Q}_1$ e $\mathbf{Q}_2$ tais que:
\begin{itemize}
    \item $\mathbf{Q}_1 + \mathbf{Q}_2 + \mathbf{P}_0 = \mathbf{I}$, onde $\mathbf{P}_0$ projeta no espaço da média geral
    \item $\mathbf{Q}_1$ corresponde à projeção no espaço dos efeitos dos tratamentos, com $\text{rank}(\mathbf{Q}_1) = t-1$
    \item $\mathbf{Q}_2$ corresponde à projeção no espaço residual, com $\text{rank}(\mathbf{Q}_2) = t(r-1)$
    \item $\mathbf{Q}_1\mathbf{Q}_2 = \mathbf{0}$, garantindo ortogonalidade e independência
\end{itemize}

Aplicando o Teorema de Cochran, segue que:
\[
\frac{SQ_E}{\sigma^2} = \frac{\mathbf{y}^T\mathbf{Q}_2\mathbf{y}}{\sigma^2} \sim \chi^2_{t(r-1)},
\]
pois $\mathbf{Q}_2$ projeta no espaço residual onde a média é zero. 

Sob $H_0$ ($\tau_1 = \cdots = \tau_t = 0$), temos $\mathbf{Q}_1\boldsymbol{\mu} = \mathbf{0}$, logo:
\[
\frac{SQ_A}{\sigma^2} = \frac{\mathbf{y}^T\mathbf{Q}_1\mathbf{y}}{\sigma^2} \sim \chi^2_{t-1}.
\]

A independência entre $SQ_A$ e $SQ_E$ decorre diretamente da ortogonalidade entre $\mathbf{Q}_1$ e $\mathbf{Q}_2$ ($\mathbf{Q}_1\mathbf{Q}_2 = \mathbf{0}$), garantida pela estrutura de projeções ortogonais no modelo linear, conforme estabelecido pela decomposição da Seção 4.

\section{Estatística F}

Como $SQ_A$ e $SQ_E$ são independentes e seguem distribuições $\chi^2$ com graus de liberdade $t-1$ e $t(r-1)$ respectivamente, a razão de suas médias é:
\[
F=\frac{MS_A}{MS_E}
= \frac{SQ_A/(t-1)}{SQ_E/[t(r-1)]}
= \frac{(SQ_A/\sigma^2)/(t-1)}{(SQ_E/\sigma^2)/[t(r-1)]}.
\]
Como a estatística $F$ é a razão entre duas variáveis qui-quadrado independentes divididas por seus graus de liberdade, ela elimina o parâmetro de escala desconhecido $\sigma^2$, tornando-se uma \textbf{estatística pivotal}. Sob $H_0$,
\[
F\sim F_{t-1,\;t(r-1)},
\]
sendo a distribuição completamente especificada e independente de parâmetros desconhecidos. Valores grandes de $F$ indicam rejeição de $H_0$, pois a variância explicada pelos tratamentos ($MS_A$) é significativamente maior que a variância residual ($MS_E$).

\section{Esperanças dos Quadrados Médios}

\[
E[MS_E]=\sigma^2,\qquad
E[MS_A] = \sigma^2 + \frac{r}{t-1}\sum_{i=1}^t \tau_i^2.
\]
Sob $H_0$, as esperanças são iguais; sob $H_1$, $MS_A$ tende a ser maior, justificando o uso do teste F.

\section{Tabela Geral da ANOVA}

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
Fonte & GL & SQ & QM & F \\
\midrule
Tratamentos & $t-1$ & $SQ_A$ & $MS_A$ & $MS_A/MS_E$ \\
Erro & $t(r-1)$ & $SQ_E$ & $MS_E$ & \\
Total & $tr-1$ & $SQ_T$ & & \\
\bottomrule
\end{tabular}
\end{table}

\section{Exemplo Aplicado}

Dados de produtividade (quatro variedades, cinco repetições):

\begin{table}[h]
    \centering
    \begin{tabular}{cccc}
        \toprule
        A & B & C & D \\
        \midrule
        25 & 31 & 22 & 33 \\
        26 & 25 & 26 & 29 \\
        20 & 28 & 28 & 31 \\
        23 & 27 & 25 & 34 \\
        21 & 24 & 29 & 28 \\
        \midrule
        Totais: 115 & 135 & 130 & 155 \\
        Médias: 23 & 27 & 26 & 31 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection*{Verificação de Pressupostos}

Antes de aplicar o teste F, é necessário verificar os pressupostos. Testes de normalidade (Shapiro–Wilk) e homogeneidade de variâncias (Levene) podem ser aplicados aos resíduos. Para este exemplo, assumimos que os pressupostos são satisfeitos.

\subsection*{Cálculos}

A soma de quadrados total é:
\[
SQT = \sum_{i=1}^4 \sum_{j=1}^5 y_{ij}^2 - \frac{(\sum_{i=1}^4 \sum_{j=1}^5 y_{ij})^2}{20} = 14587 - \frac{535^2}{20} = 275{,}75.
\]
A soma de quadrados entre tratamentos é:
\[
SQA = \frac{1}{5}(115^2 + 135^2 + 130^2 + 155^2) - \frac{535^2}{20} = \frac{72375}{5} - 14311{,}25 = 14475 - 14311{,}25 = 163{,}75.
\]
A soma de quadrados residual é:
\[
SQE = SQT - SQA = 275{,}75 - 163{,}75 = 112{,}00.
\]
Os quadrados médios são: $MS_A = SQA/3 = 54{,}583$ e $MS_E = SQE/16 = 7{,}000$.

\subsection*{Tabela ANOVA}

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
Fonte & GL & SQ & QM & F \\
\midrule
Tratamentos & 3 & 163{,}75 & 54{,}583 & 7{,}798 \\
Erro & 16 & 112{,}00 & 7{,}000 & \\
Total & 19 & 275{,}75 & & \\
\bottomrule
\end{tabular}
\end{table}

\subsection*{Conclusão}

Como $F_{calc}=7{,}798 > F_{crit}=3{,}24$ ($\alpha=0{,}05$), rejeitamos $H_0$. Há evidência suficiente de diferença significativa entre as variedades ao nível de 5\% de significância. A variedade D apresentou maior produtividade média (31), indicando que os tratamentos diferem estatisticamente.

\begin{thebibliography}{9}
\bibitem{fisher1935}
FISHER, R. A. \textit{The design of experiments}. 1935.

\bibitem{box1978}
BOX, G. E. P. et al. \textit{Statistics for experimenters}. Wiley, 1978.

\bibitem{snedecor1989}
SNEDECOR, G. W.; COCHRAN, W. G. \textit{Statistical methods}. Iowa State, 1989.
\end{thebibliography}

\end{document}
